{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "## 概述\n",
    "  在目前的深度学习任务，特别是NLP任务中，现在大多说的模型都会有attention的存在，在17年的attention is all you need论文出来之后，陆陆续续的出现了Bert，GPT等强大的模型以及相应的变种，在这些模型中传统的RNNs和CNN已经被attention完全取代(详细架构参考相关论文)，由此可见在现在的NLP领域内，attention绝对拥有一个非常重要的地位，在这篇文章中，我将详细的描述和attention相关的基本概念，也会阐述具体的实现细节，这些实现细节往往是我自己结合实践的过程和一些优质的博客后总结的，比起大多数文章中只是总结性的介绍原理，我将会分为两个部分进行阐述，前半部分是基本概念和原理，满足想要大概了解的读者，后半部分将会结合实例细致分析如何实现，满足想要在自己的实践中应用attention的读者。\n",
    "  \n",
    "## 文章结构\n",
    "    \n",
    "    1. 第一部分(概念、原理)\n",
    "        总体架构\n",
    "        attention分类\n",
    "    2. 第二部分(实践细节)\n",
    "    \n",
    "    \n",
    "## 第一部分\n",
    "\n",
    "### attention function\n",
    "    下面的几种attention函数都是比较主流的attention函数，可以计算向量之间的相关性，具体的就是可以计算queary和key之间的相关性，不同的attention funcition有不同的计算方式，实现起来也有不同之处，下面将会细致的介绍每种attention function的处理细节。\n",
    "\n",
    "#### additive attention\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
