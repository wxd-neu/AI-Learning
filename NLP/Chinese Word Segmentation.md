# 中文分词

## 一 概述

  中文分词(word segmentation)是中文信息处理的第一步，也是非常重要的一步，其任务简单的来说就是把中文的句子切分词级别，也可以说是对于中文句子中的词与词之间加上标记分割，当然在具体的实现中还是存在一些挑战，如分词规范、歧义切分和新词识别等。需要注意的是近些年由于如bert等复杂模型的出现，对于很多任务其实不需要分词，直接把每个字作为处理的最小单元也是可以的。
  本文先介绍一些和中文分词相关的基本概念，接着介绍分词的基本原理，最后介绍几个比较常用的分词工具。

## 二 基本概念

### 1 词、词组、句子

  这三个概念从字面上都能有一个大概的了解，下面给出它们不太规范的含义
  词：词是语言中最小的处理单元，词的划分具有一定的主观性，如“中国北京”可以当做是一个词，也可以视为“中国”和“北京”两个词
  词组：我们把语义和语法上都能够搭配的两个及以上的词的组合成为词组，简单来说就是多个词组合在一起没有语法错误，语义上可说的通便可称之为一个
       词组，如上面的例子中的“中国北京”或者是“计算机科学与技术”
  句子：简单的说句子就是由多个词组成的能够表达一个完整意思的语言单位。

### 2 语言模型(LM)

  由于语言模型是一个比较大的概念，其中涉及到许多的细节，这里只是简单的介绍一些基本的概念，如果想要详细了解，可以参考我的另外一篇文章。

  #### LM的概念
  由语音、词汇、语法构成的交流模型

  #### 语言模型的种类

      文法语言模型         典型代表：短语结构语法PSG
      统计语言模型         典型代表：n元语法模型n-gram
      深度学习语言模型      典型代表：神经网络语言模型NNLM
      Attention语言模型    典型代表：Masked Language Model

  #### 语言模型简介

  在上述的四种语言模型中，我个人认为发展到现在，对于第一种如果不了解也不会有多么大的影响，所以接下来我会简单介绍其余的三种语言模型。(当然
  这只是我个人的拙见，如果有大佬不同意我也表示理解，哈哈)

#### (1) n-gram LM

在统计语言模型中，自然语言被当做一个随机过程，其中每一个语言单位包括字、词、句子、段落和篇章等都看做有一定概率分布的随机变量。

下面来看一个具体的例子，如果一个句子S由若干个词组成，如何求P(S)？  
 假设句子 S 由 w<sub>1</sub>, w<sub>2</sub>,... , w<sub>n</sub> 组成直接去计算P(S)将会很困难(why?具体原因可以看我的另一篇文章)，在统计语言模型中我们通常通过计算离散的条    件概率乘积来计算P(S)，公式如下：
$$
P(S) =     P(w_1, w_2, ...,w_n)= \prod_{i = 1}^{n}P(w_i|h_i)
$$
公式中的h<sub>i</sub>为{w<sub>1</sub>,w<sub>2</sub>,...w<sub>i-1</sub>,w<sub>i+1</sub>,...w<sub>n</sub>}，实际上就是w<sub>i</sub>的上下文，由马尔科夫链可以对公式进行简化，对于实际的应用中也是这么处理的，当前的w<sub>i</sub>只与其前面若干个词相关，因此得出以下常用的n-gram模型：  ![n-gram table](https://github.com/wxd-neu/AI-Learning/blob/master/Source/Pictures/n-gram.png)



